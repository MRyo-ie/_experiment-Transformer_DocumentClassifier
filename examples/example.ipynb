{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "import os \n",
    "import random\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import DocumentClassifierUsingBertJapanese as clf\n",
    "%autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>待望の…待望のUSB経由のネットワーク接続が可能に！！Android情報サイト「AppCom...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>GALAXY SIII SC-06Dが発表！NTTドコモは16日、今夏に発売する予定の新モデ...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>女子サッカーアメリカ代表のFWアレックス・モーガン（22歳）が、米誌『スポーツ・イラストレイ...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>プロボクサー・亀田興毅が、かねてから語っていた“30歳で引退”という目指すべき現役のゴールを...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>GALAXY SII SC-02CにAndroid 4.0 ICSが提供開始！NTTドコモは...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Text  Label\n",
       "0  待望の…待望のUSB経由のネットワーク接続が可能に！！Android情報サイト「AppCom...      3\n",
       "1  GALAXY SIII SC-06Dが発表！NTTドコモは16日、今夏に発売する予定の新モデ...      3\n",
       "2  女子サッカーアメリカ代表のFWアレックス・モーガン（22歳）が、米誌『スポーツ・イラストレイ...      2\n",
       "3  プロボクサー・亀田興毅が、かねてから語っていた“30歳で引退”という目指すべき現役のゴールを...      2\n",
       "4  GALAXY SII SC-02CにAndroid 4.0 ICSが提供開始！NTTドコモは...      3"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"./data/train_eval.tsv\", sep='\\t')\n",
    "# print(df)\n",
    "\n",
    "le = LabelEncoder()\n",
    "# df['label'] = le.fit_transform(df.label.values)\n",
    "# df = df[['text', 'label']]\n",
    "df.columns = ['Text', 'Label']\n",
    "train_df, val_df = train_test_split(df, test_size=0.2, stratify=df.Label)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at cl-tohoku/bert-base-japanese-whole-word-masking were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at cl-tohoku/bert-base-japanese-whole-word-masking and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = clf.DocumentClassifier(num_labels=9, num_epochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 210328 12:52:15 DocumentClassifierUsingBertJapanese:172] [Start]Create DataSets from DataFrames\n",
      "[I 210328 12:52:45 DocumentClassifierUsingBertJapanese:175] [Finished]Create DataSets from DataFrames\n",
      "[I 210328 12:52:45 DocumentClassifierUsingBertJapanese:179] [Start]Create DataLoaders\n",
      "[I 210328 12:52:45 DocumentClassifierUsingBertJapanese:182] [Finished]Create DataLoaders\n",
      "[I 210328 12:52:45 DocumentClassifierUsingBertJapanese:227] 使用デバイス：cuda:0\n",
      "[I 210328 12:52:45 DocumentClassifierUsingBertJapanese:228] -----start-------\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "max() received an invalid combination of arguments - got (str, int), but expected one of:\n * (Tensor input)\n * (Tensor input, name dim, bool keepdim, *, tuple of Tensors out)\n * (Tensor input, Tensor other, *, Tensor out)\n * (Tensor input, int dim, bool keepdim, *, tuple of Tensors out)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-37-2e213980159b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_df\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_df\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mearly_stopping_rounds\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/_p/ZEN/Nami/slackbot_ML_model_tester/start_slackbot/botmodules/DocumentClassification_BERT-Japanese/DocumentClassifierUsingBertJapanese.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, train_df, val_df, early_stopping_rounds, fine_tuning_type)\u001b[0m\n\u001b[1;32m    216\u001b[0m         self.net = self._train_model(\n\u001b[1;32m    217\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnet\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataloaders_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_epochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_epochs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 218\u001b[0;31m             patience=early_stopping_rounds)\n\u001b[0m\u001b[1;32m    219\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    220\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/_p/ZEN/Nami/slackbot_ML_model_tester/start_slackbot/botmodules/DocumentClassification_BERT-Japanese/DocumentClassifierUsingBertJapanese.py\u001b[0m in \u001b[0;36m_train_model\u001b[0;34m(net, dataloaders_dict, criterion, optimizer, num_epochs, patience)\u001b[0m\n\u001b[1;32m    276\u001b[0m                         \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    277\u001b[0m                         \u001b[0;31m#loss = criterion(outputs, labels)  # 損失を計算\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 278\u001b[0;31m                         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogit\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    279\u001b[0m                         \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpreds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogit\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# ラベルを予測\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    280\u001b[0m                         \u001b[0mpredictions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpreds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: max() received an invalid combination of arguments - got (str, int), but expected one of:\n * (Tensor input)\n * (Tensor input, name dim, bool keepdim, *, tuple of Tensors out)\n * (Tensor input, Tensor other, *, Tensor out)\n * (Tensor input, int dim, bool keepdim, *, tuple of Tensors out)\n"
     ]
    }
   ],
   "source": [
    "model.fit(train_df, val_df, early_stopping_rounds=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 210328 12:44:21 DocumentClassifierUsingBertJapanese:329] [Start]Create DataSet, DataLoader from DataFrame\n",
      "[I 210328 12:44:27 DocumentClassifierUsingBertJapanese:334] [Finished]Create DataSet, DataLoader from DataFrame\n",
      "[I 210328 12:44:27 DocumentClassifierUsingBertJapanese:337] 使用デバイス：cuda:0\n",
      "[I 210328 12:44:27 DocumentClassifierUsingBertJapanese:338] -----start-------\n",
      "100%|██████████| 37/37 [01:56<00:00,  3.14s/it]\n",
      "[I 210328 12:46:23 DocumentClassifierUsingBertJapanese:348] -----finished-------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0.10159193, 0.12381991, 0.07763703, ..., 0.12575388, 0.11685082,\n",
       "        0.14893092],\n",
       "       [0.09534844, 0.1253567 , 0.10198842, ..., 0.10485069, 0.13035609,\n",
       "        0.15408024],\n",
       "       [0.10034444, 0.11472436, 0.11446261, ..., 0.10865485, 0.10873953,\n",
       "        0.14498372],\n",
       "       ...,\n",
       "       [0.10490467, 0.11947428, 0.08808235, ..., 0.11664749, 0.10675719,\n",
       "        0.14918758],\n",
       "       [0.10365929, 0.12079606, 0.0910448 , ..., 0.1206104 , 0.10771292,\n",
       "        0.14538683],\n",
       "       [0.10329206, 0.11612666, 0.09550446, ..., 0.12401586, 0.11218169,\n",
       "        0.1439864 ]], dtype=float32)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_proba = model.predict(val_df)\n",
    "y_proba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1181, 9)\n",
      "[8 8 5 ... 5 5 5]\n"
     ]
    }
   ],
   "source": [
    "print(y_proba.shape)\n",
    "print(y_proba.argmax(axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['私', 'は', '、', '火星', 'という', '星', 'まで', '宇宙', '旅行', 'し', 'て', 'みたい', 'です', '。']\n",
      "[1325, 9, 6, 4, 140, 1414, 126, 1669, 3807, 15, 16, 17131, 2992, 8]\n",
      "tensor([[ 1325,     9,     6,     4,   140,  1414,   126,  1669,  3807,    15,\n",
      "            16, 17131,  2992,     8]])\n"
     ]
    }
   ],
   "source": [
    "text = '私は、火星という星まで宇宙旅行してみたいです。'\n",
    "tokenized_text = model.tokenizer.tokenize(text)\n",
    "print(tokenized_text)\n",
    "\n",
    "#ここでは「火星」をMASKして、「火星」以外の単語を探そうとしている\n",
    "masked_index = 3\n",
    "tokenized_text[masked_index] = '[MASK]'\n",
    "\n",
    "# Convert token to vocabulary indices\n",
    "indexed_tokens = model.tokenizer.convert_tokens_to_ids(tokenized_text)\n",
    "print(indexed_tokens)\n",
    "\n",
    "tokens_tensor = torch.tensor([indexed_tokens])\n",
    "print(tokens_tensor)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelWithLMHead"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/transformers/models/auto/modeling_auto.py:1010: FutureWarning: The class `AutoModelWithLMHead` is deprecated and will be removed in a future version. Please use `AutoModelForCausalLM` for causal language models, `AutoModelForMaskedLM` for masked language models and `AutoModelForSeq2SeqLM` for encoder-decoder models.\n",
      "  FutureWarning,\n",
      "Some weights of the model checkpoint at cl-tohoku/bert-base-japanese-whole-word-masking were not used when initializing BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'DocumentClassifier' object has no attribute 'eval'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-3b54da11d194>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mmodel_auto\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAutoModelWithLMHead\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"cl-tohoku/bert-base-japanese-whole-word-masking\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'DocumentClassifier' object has no attribute 'eval'"
     ]
    }
   ],
   "source": [
    "model_auto = AutoModelWithLMHead.from_pretrained(\"cl-tohoku/bert-base-japanese-whole-word-masking\")\n",
    "model.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Predict\n",
    "with torch.no_grad():\n",
    "    outputs = model(tokens_tensor)\n",
    "    #データ構成 tapleの1個目固定、 [1, 14, 32000] 1個の文章、14つの単語、32000次元の数値\n",
    "    predictions = outputs[0][0, masked_index].topk(7) # 予測結果の上位7件を抽出\n",
    "\n",
    "print(outputs[0].shape)\n",
    "print(predictions.indices)\n",
    "\n",
    "#「火星」と置き換える文字候補を出す\n",
    "for i, index_t in enumerate(predictions.indices):\n",
    "    index = index_t.item()\n",
    "\n",
    "    #数値を元の単語に戻す\n",
    "    token = model.tokenizer.convert_ids_to_tokens([index ])[0]\n",
    "    print(i, token)\n",
    "\n",
    "    tokenized_text[masked_index] = token\n",
    "    print(\"\".join(tokenized_text))\n",
    "\n",
    "#たしかにそれらしい文章が出力されました。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'keras_bert'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-7890e5a7b902>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mload_model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreprocessing\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msequence\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpad_sequences\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras_bert\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mload_trained_model_from_checkpoint\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras_bert\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mget_custom_objects\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mclassification_report\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfusion_matrix\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'keras_bert'"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import pandas as pd\n",
    "import sentencepiece as spm\n",
    "import logging\n",
    "import numpy as np\n",
    "\n",
    "from keras import utils\n",
    "from keras.models import load_model\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras_bert import load_trained_model_from_checkpoint\n",
    "from keras_bert import get_custom_objects\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "\n",
    "sys.path.append('modules')\n",
    "\n",
    "# SentencePieceProccerモデルの読込\n",
    "spp = spm.SentencePieceProcessor()\n",
    "spp.Load('/content/drive/My Drive/bert/bert-wiki-ja/wiki-ja.model')\n",
    "# BERTの学習したモデルの読込\n",
    "model_filename = '/content/drive/My Drive/bert/models/knbc_finetuning.model'\n",
    "model = load_model(model_filename, custom_objects=get_custom_objects())\n",
    "\n",
    "SEQ_LEN = 103\n",
    "maxlen = SEQ_LEN\n",
    "\n",
    "def _get_indice(feature):\n",
    "    indices = np.zeros((maxlen), dtype=np.int32)\n",
    "\n",
    "    tokens = []\n",
    "    tokens.append('[CLS]')\n",
    "    tokens.extend(spp.encode_as_pieces(feature))\n",
    "    tokens.append('[SEP]')\n",
    "\n",
    "    for t, token in enumerate(tokens):\n",
    "        if t >= maxlen:\n",
    "            break\n",
    "        try:\n",
    "            indices[t] = spp.piece_to_id(token)\n",
    "        except:\n",
    "            logging.warn('unknown')\n",
    "            indices[t] = spp.piece_to_id('<unk>')\n",
    "    return indices\n",
    "\n",
    "feature = \"昨日は携帯電話を買いに行った。\"\n",
    "\n",
    "test_features = []\n",
    "test_features.append(_get_indice(feature))\n",
    "test_segments = np.zeros(\n",
    "    (len(test_features), maxlen), dtype=np.float32)\n",
    "\n",
    "predicted_test_labels = model.predict(\n",
    "    [test_features, test_segments]).argmax(axis=1)\n",
    "\n",
    "label_data = pd.read_csv('/content/drive/My Drive/bert/label_id/id_category.csv')\n",
    "label = label_data.query(f'id == {predicted_test_labels[0]}')\n",
    "label = label.iloc[0]\n",
    "label_name = label['label']\n",
    "print(label_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
