{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 試し\n",
    "- [Huggingface Transformers 入門 (24) - 日本語の言語モデルの学習](https://note.com/npaka/n/n0a2d0a4b806e#x22Z6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers[ja] in /usr/local/lib/python3.6/dist-packages (4.4.2)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers[ja]) (3.0.12)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.6/dist-packages (from transformers[ja]) (1.18.5)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers[ja]) (2021.3.17)\n",
      "Requirement already satisfied: dataclasses in /usr/local/lib/python3.6/dist-packages (from transformers[ja]) (0.8)\n",
      "Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /usr/local/lib/python3.6/dist-packages (from transformers[ja]) (0.10.1)\n",
      "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.6/dist-packages (from transformers[ja]) (3.7.3)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from transformers[ja]) (20.8)\n",
      "Requirement already satisfied: sacremoses in /usr/local/lib/python3.6/dist-packages (from transformers[ja]) (0.0.43)\n",
      "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers[ja]) (4.59.0)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers[ja]) (2.25.1)\n",
      "Requirement already satisfied: fugashi>=1.0 in /usr/local/lib/python3.6/dist-packages (from transformers[ja]) (1.1.0)\n",
      "Requirement already satisfied: ipadic<2.0,>=1.0.0 in /usr/local/lib/python3.6/dist-packages (from transformers[ja]) (1.0.0)\n",
      "Requirement already satisfied: unidic-lite>=1.0.7 in /usr/local/lib/python3.6/dist-packages (from transformers[ja]) (1.0.8)\n",
      "Collecting unidic>=1.0.2\n",
      "  Downloading unidic-1.0.3.tar.gz (5.1 kB)\n",
      "Collecting wasabi<1.0.0,>=0.6.0\n",
      "  Downloading wasabi-0.8.2-py3-none-any.whl (23 kB)\n",
      "Collecting plac<2.0.0,>=1.1.3\n",
      "  Downloading plac-1.3.3-py2.py3-none-any.whl (22 kB)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers[ja]) (2020.12.5)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers[ja]) (1.26.4)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers[ja]) (4.0.0)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers[ja]) (2.10)\n",
      "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata->transformers[ja]) (3.7.4.3)\n",
      "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata->transformers[ja]) (3.4.1)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /usr/lib/python3/dist-packages (from packaging->transformers[ja]) (2.2.0)\n",
      "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers[ja]) (7.1.2)\n",
      "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers[ja]) (1.0.0)\n",
      "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers[ja]) (1.15.0)\n",
      "Building wheels for collected packages: unidic\n",
      "  Building wheel for unidic (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for unidic: filename=unidic-1.0.3-py3-none-any.whl size=5498 sha256=bd7261af99ae0a15f543af1f66989b36068cc3353d5fca0a7b9a890d9e54b608\n",
      "  Stored in directory: /home/mitoro18/.cache/pip/wheels/ee/f8/fc/b3cf448b2c882c609db23ff164ab8d07b447d7ca69c1c294de\n",
      "Successfully built unidic\n",
      "Installing collected packages: wasabi, plac, unidic\n",
      "Successfully installed plac-1.3.3 unidic-1.0.3 wasabi-0.8.2\n"
     ]
    }
   ],
   "source": [
    "!pip3 install transformers['ja']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import BertJapaneseTokenizer, AutoModelForMaskedLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at cl-tohoku/bert-base-japanese-whole-word-masking were not used when initializing BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "# トークナイザーとモデルの準備\n",
    "tokenizer = BertJapaneseTokenizer.from_pretrained('cl-tohoku/bert-base-japanese-whole-word-masking')\n",
    "model = AutoModelForMaskedLM.from_pretrained('cl-tohoku/bert-base-japanese-whole-word-masking')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(input_ids):\n",
    "    # テキストをテンソルに変換\n",
    "    input_ids = tokenizer.encode(text, return_tensors='pt')\n",
    "    masked_index = torch.where(input_ids == tokenizer.mask_token_id)[1].tolist()[0]\n",
    "\n",
    "    # 推論\n",
    "    result = model(input_ids)\n",
    "    pred_ids = result[0][:, masked_index].topk(5).indices.tolist()[0]\n",
    "    for pred_id in pred_ids:\n",
    "        output_ids = input_ids.tolist()[0]\n",
    "        output_ids[masked_index] = pred_id\n",
    "        print(tokenizer.decode(output_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CLS] 吾輩 は し で ある 。 名前 は まだ ない 。 [SEP]\n",
      "[CLS] 吾輩 は 一 で ある 。 名前 は まだ ない 。 [SEP]\n",
      "[CLS] 吾輩 は と で ある 。 名前 は まだ ない 。 [SEP]\n",
      "[CLS] 吾輩 は 数 で ある 。 名前 は まだ ない 。 [SEP]\n",
      "[CLS] 吾輩 は 生まれ で ある 。 名前 は まだ ない 。 [SEP]\n"
     ]
    }
   ],
   "source": [
    "# テキスト\n",
    "text = f'吾輩は{tokenizer.mask_token}である。名前はまだない。'\n",
    "\n",
    "predict(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CLS] テレビ で し の 試合 を 見る 。 [SEP]\n",
      "[CLS] テレビ で 一 の 試合 を 見る 。 [SEP]\n",
      "[CLS] テレビ で と の 試合 を 見る 。 [SEP]\n",
      "[CLS] テレビ で 生まれ の 試合 を 見る 。 [SEP]\n",
      "[CLS] テレビ で 元 の 試合 を 見る 。 [SEP]\n"
     ]
    }
   ],
   "source": [
    "# Tokenize input\n",
    "text = f'テレビで{tokenizer.mask_token}の試合を見る。'\n",
    "# tokenized_text = tokenizer.tokenize(text)\n",
    "# # ['テレビ', 'で', 'サッカー', 'の', '試合', 'を', '見る', '。']\n",
    "\n",
    "# # Mask a token that we will try to predict back with `BertForMaskedLM`\n",
    "# masked_index = 2\n",
    "# tokenized_text[masked_index] = '[MASK]'\n",
    "# # ['テレビ', 'で', '[MASK]', 'の', '試合', 'を', '見る', '。']\n",
    "\n",
    "predict(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
